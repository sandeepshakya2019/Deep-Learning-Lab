# -*- coding: utf-8 -*-
"""Multi Class Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13_1u0YOl_DLZbisz0nBLP7WvTKTS6CcL

# Logistic Regression on Digits Data
"""

# Commented out IPython magic to ensure Python compatibility.
# REF - https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html

print(__doc__)

# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>
# License: BSD 3 clause

# Standard scientific Python imports
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Import datasets, classifiers and performance metrics
from sklearn import datasets, svm, metrics

# The digits dataset
digits = datasets.load_digits()

# The data that we are interested in is made of 8x8 images of digits, let's
# have a look at the first 4 images, stored in the `images` attribute of the
# dataset.  If we were working from image files, we could load them using
# matplotlib.pyplot.imread.  Note that each image must have the same size. For these
# images, we know which digit they represent: it is given in the 'target' of
# the dataset.
images_and_labels = list(zip(digits.images, digits.target))
for index, (image, label) in enumerate(images_and_labels[:4]):
    plt.subplot(2, 4, index + 1)
    plt.axis('off')
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    plt.title('Training: %i' % label)

# To apply a classifier on this data, we need to flatten the image, to
# turn the data in a (samples, feature) matrix:
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))

# Create a classifier: a support vector classifier
#classifier = svm.SVC(gamma=0.001)
classifier = LogisticRegression()

# We learn the digits on the first half of the digits
classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])

# Now predict the value of the digit on the second half:
expected = digits.target[n_samples // 2:]
predicted = classifier.predict(data[n_samples // 2:])

print("Classification report for classifier %s:\n%s\n"
#       % (classifier, metrics.classification_report(expected, predicted)))
print("Confusion matrix:\n%s" % metrics.confusion_matrix(expected, predicted))

images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))
for index, (image, prediction) in enumerate(images_and_predictions[:4]):
    plt.subplot(2, 4, index + 5)
    plt.axis('off')
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    plt.title('Prediction: %i' % prediction)

plt.show()

"""# PR Metrics

### Reference material

1. REF - https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html

1. REF - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score

1. REF - https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics

1. REF - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin

### Data
"""

# Load digits data

X = digits.data
y = digits.target

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4)

print (X_test.shape, y_test.shape)

"""### One hot encoding"""

from sklearn.preprocessing import label_binarize
import numpy as np

my_classes = np.unique(y)

y1_test = label_binarize(y_test,classes=my_classes)

print (y.shape, y1_test.shape)

"""### Fit classifier"""

clf = LogisticRegression()

clf.fit(X_train,y_train)

y_pred = clf.predict(X_test)
y_probas = clf.predict_proba(X_test)

print (y_pred.shape, y_probas.shape)

"""## PR Curves Per Class"""

from sklearn.metrics import precision_recall_curve

plt.figure()

for mycls in my_classes :
  p,r,t = precision_recall_curve(y_true=y1_test[:,mycls],probas_pred=y_probas[:,mycls])
  plt.plot(r,p,label=str(mycls))

plt.legend(loc='best')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Per class PR curves')
plt.show()

"""# Class Specific Average Precision Scores"""

for mycls in range(10) :
    avg_prec = average_precision_score(y_true=y1_test[:,mycls],y_score=y_probas[:,mycls])
    print (mycls, avg_prec)

"""# Mean Average Precision"""

from sklearn.metrics import average_precision_score

map_score = average_precision_score(y_true=y1_test,y_score=y_probas)
avgwt_map_score = average_precision_score(y_true=y1_test,y_score=y_probas,average='weighted')

print (map_score, avgwt_map_score)