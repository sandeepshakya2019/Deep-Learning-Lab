# -*- coding: utf-8 -*-
"""adaboost_generated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gx0WrCv8w6V3WgK6AWAdHn17-86_QeKZ

# Ada boost on make moons, blobs and circles

##Ada boost with logistic regression
"""

from sklearn.datasets import make_blobs, make_circles, make_moons
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, classification_report, precision_score, average_precision_score
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier

# Function to train and evaluate AdaBoost classifier
def train_and_evaluate_adaboost(X_train, y_train, X_test, y_test, base_estimator, n_estimators, dataset_name):
    clf = AdaBoostClassifier(estimator=base_estimator, n_estimators=n_estimators, random_state=42)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    classification_rep = classification_report(y_test, y_pred)


    print("Accuracy:", accuracy)
    print("Precision:", precision)
    print("Classification Report:")
    print(classification_rep)

    classes = np.unique(y_test)
    mean_avg_precision = 0

    for class_label in classes:

        binary_y_test = (y_test == class_label).astype(int)
        binary_y_pred = (y_pred == class_label).astype(int)

        avg_precision = average_precision_score(binary_y_test, binary_y_pred)
        print(f"Average Precision for Class {class_label}: {avg_precision:.4f}")

        mean_avg_precision += avg_precision

    mean_avg_precision /= 3
    print(f"\nMean Average Precision: {mean_avg_precision:.4f}")

    # Evaluate the classifier
    train_score = clf.score(X_train, y_train)
    test_score = clf.score(X_test, y_test)

    print(f"Dataset: {dataset_name} | Base Estimator: {base_estimator.__class__.__name__} | Estimators: {n_estimators} | Train Score: {train_score:.4f} | Test Score: {test_score:.4f}")

datasets = [
    make_blobs(n_samples=100, centers=5, random_state=42),
    make_circles(n_samples=100, noise=0.1, factor=0.5, random_state=42),
    make_moons(n_samples=100, noise=0.2, random_state=42)
]

base_estimators = [
    LogisticRegression(),
]

n_estimators = [1, 10, 20, 50, 70, 100, 200, 300]
for dataset_idx, (X, y) in enumerate(datasets):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    for base_estimator in base_estimators:
        for n_estimator in n_estimators:
            train_and_evaluate_adaboost(X_train, y_train, X_test, y_test, base_estimator, n_estimator, f"Dataset {dataset_idx + 1}")
    print()
    print()

X, y =    make_blobs(n_samples=100, centers=5, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4)

ada_clf = AdaBoostClassifier(estimator=LogisticRegression(), n_estimators=50, random_state=42)
ada_clf.fit(X_train, y_train)
train_score = ada_clf.score(X_train, y_train)
test_score = ada_clf.score(X_test, y_test)
print(train_score,test_score)

ada_clf = AdaBoostClassifier(estimator=LogisticRegression(), n_estimators=200, random_state=42)
ada_clf.fit(X_train, y_train)
train_score = ada_clf.score(X_train, y_train)
test_score = ada_clf.score(X_test, y_test)
print(train_score,test_score)

"""## Ada Boost with DTrees:"""

datasets = [
    make_blobs(n_samples=100, centers=5, random_state=42),
    make_circles(n_samples=100, noise=0.1, factor=0.5, random_state=42),
    make_moons(n_samples=100, noise=0.2, random_state=42)
]
#MAX DEPTH = 1
base_estimators = [
    DecisionTreeClassifier(max_depth=1)
]

n_estimators = [1, 10, 20, 50, 70, 100, 200, 300]
for dataset_idx, (X, y) in enumerate(datasets):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    for base_estimator in base_estimators:
        for n_estimator in n_estimators:
            train_and_evaluate_adaboost(X_train, y_train, X_test, y_test, base_estimator, n_estimator, f"Dataset {dataset_idx + 1}")
    print()
    print()

#MAX DEPTH = 2
base_estimators = [
    DecisionTreeClassifier(max_depth=2)
]

n_estimators = [1, 10, 20, 50, 70, 100, 200, 300]
for dataset_idx, (X, y) in enumerate(datasets):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    for base_estimator in base_estimators:
        for n_estimator in n_estimators:
            train_and_evaluate_adaboost(X_train, y_train, X_test, y_test, base_estimator, n_estimator, f"Dataset {dataset_idx + 1}")
    print()
    print()