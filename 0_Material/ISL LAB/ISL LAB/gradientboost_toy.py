# -*- coding: utf-8 -*-
"""gradientBoost_toy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AHJfMJnP20VAZ2JgGu8Wes4kMumfmkvH
"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report
import numpy as np

def calculate_mean_average_precision(y_true, y_pred):
    unique_classes = np.unique(y_true)
    avg_precisions = []

    for cls in unique_classes:
        # Get binary labels for the current class
        binary_true = np.where(y_true == cls, 1, 0)
        binary_pred = np.where(y_pred == cls, 1, 0)

        # Calculate precision for the current class
        precision = precision_score(binary_true, binary_pred, zero_division=1)

        # Append precision to list of average precisions
        avg_precisions.append(precision)

    mean_avg_precision = np.mean(avg_precisions)
    return mean_avg_precision

def evaluate_classifier(clf, X_train, y_train, X_test, y_test):
    clf.fit(X_train, y_train)
    y_train_pred = clf.predict(X_train)
    y_test_pred = clf.predict(X_test)

    train_accuracy = accuracy_score(y_train, y_train_pred)
    test_accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred, average='macro', zero_division=1)
    recall = recall_score(y_test, y_test_pred, average='macro', zero_division=1)
    classification_matrix = classification_report(y_test, y_test_pred,zero_division=1)
    mean_avg_precision = calculate_mean_average_precision(y_test, y_test_pred)

    print(f"Train Accuracy: {train_accuracy:.4f} | Test Accuracy: {test_accuracy:.4f}")
    print(f"Precision: {precision:.4f} | Recall: {recall:.4f}")
    print("Classification Report:\n", classification_matrix)
    print("Mean Average Precision:", mean_avg_precision)

# Load Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Gradient Boosting Classifier with different numbers of estimators
print("Gradient Boosting Classifier with different numbers of estimators:")
estimators_list = [1, 10, 20, 30]
for n_estimators in estimators_list:
    print(f"Number of estimators: {n_estimators}")
    clf = GradientBoostingClassifier(n_estimators=n_estimators, random_state=42)
    evaluate_classifier(clf, X_train, y_train, X_test, y_test)
    print()