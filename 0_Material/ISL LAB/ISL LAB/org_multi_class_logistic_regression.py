# -*- coding: utf-8 -*-
"""Multi Class Logistic Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WieECA_bP7f5g2PdsJfMZaVd81TqU1CL

# Import Libraries
"""

# Author: Kalidas Y <ykalidas at iittp dot ac dot in>
# License: BSD 3 clause

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.linear_model import LogisticRegression
from matplotlib.colors import ListedColormap
from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, precision_recall_fscore_support
from sklearn.preprocessing import label_binarize

"""# Visualize Binary Log Odds

## Visualize Odds Ratio
"""

# Author: Kalidas Y <ykalidas at iittp dot ac dot in>
# License: BSD 3 clause

f_odds = lambda x : x/(1-x)

# Author: Kalidas Y <ykalidas at iittp dot ac dot in>
# License: BSD 3 clause

p = np.linspace(0,1,100)

plt.plot(p,f_odds(p))

plt.show()

"""## Visualize Log Odds"""

# Author: Kalidas Y <ykalidas at iittp dot ac dot in>
# License: BSD 3 clause

f_logodds = lambda x : np.log(x/(1-x))

plt.plot(p,f_logodds(p))
plt.xticks(np.arange(0,1,0.1))
plt.grid()
plt.show()

"""## Visualize P(y=+1|w,xi) for (xi,yi) over several w's"""

# Author: Kalidas Y <ykalidas at iittp dot ac dot in>
# License: BSD 3 clause

f_p_wx = lambda wx : (1/(1+np.exp(-wx)))

# Author: Kalidas Y <ykalidas at iittp dot ac dot in>
# License: BSD 3 clause

wx = np.linspace(-10,10,100)

# Author: Kalidas Y <ykalidas at iittp dot ac dot in>
# License: BSD 3 clause

plt.plot(wx,f_p_wx(wx))
plt.yticks(np.arange(0,1,0.1))
plt.grid()
plt.show()

"""# Multi Class Logistic Regression

## Create data set
"""

X, y = make_blobs(n_samples=200, n_features=2, centers=[[0,0],[0,10],[10,0]])

plt.scatter(X[:,0],X[:,1])
plt.show()

print (np.unique(y))

"""## Fit the classifier"""

clf = LogisticRegression(fit_intercept=False)

clf.fit(X,y)

clf.score(X,y)

"""## Visualize the prediction landscape"""

#REF - https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html

h = 0.02
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]

Z = Z.reshape(xx.shape)

plt.scatter(X[:,0],X[:,1],c=y,cmap=ListedColormap(['#FF0000', '#00FF00', '#0000FF']))
plt.contourf(xx, yy, Z, cmap=ListedColormap(['#FF0000', '#00FF00', '#0000FF']), alpha=.4)

plt.scatter(clf.coef_[:,0],clf.coef_[:,1],s=200,c='black')
plt.annotate('W0',xy=clf.coef_[0],color='white')
plt.annotate('W1',xy=clf.coef_[1],color='white')
plt.annotate('W2',xy=clf.coef_[2],color='white')

plt.show()

"""## Manual Prediction of Catagories"""

# Author: Kalidas Y <ykalidas at iittp dot ac dot in>
# License: BSD 3 clause

print ('X',X.shape)

y1_numr = np.exp(np.matmul(X,clf.coef_.T))
y1_denom = np.sum(y1_num,axis=1).reshape(-1,1)

print ('y1_numr',y1_numr.shape)
y1_softmax = 1.0*y1_numr/y1_denom

y_pred_manually = np.argmax(y1_softmax,axis=1)

print (np.unique(y,return_counts=True))

print (confusion_matrix(y,y_pred_manually))

print (confusion_matrix(y,clf.predict(X)))

y_pred_auto = clf.predict(X)
print ('Difference between manual and auto',np.sum(y_pred_manually - y_pred_auto))

y_pred_proba = clf.predict_proba(X)

print (y_pred_proba.shape)

print (y_pred_proba[0])

print ('Sum of probas for a point',np.sum(y_pred_proba[0]))

"""## Precision Recall Curve"""

# Author: Kalidas Y <ykalidas at iittp dot ac dot in>
# License: BSD 3 clause

def my_pr_curve(ytrue,yproba) :
    ret_p = []
    ret_r = []
    ret_t = []
    for t in np.arange(0,1,0.1) :

        #above a threshold are all 1
        ypred = np.array(yproba>=t,dtype=int)

        pred_tp = np.sum(ypred * ytrue)
        pred_p = np.sum(ypred)
        true_p = np.sum(ytrue)

        p = np.round(1.0*pred_tp/(pred_p+0.00001),2)
        r = np.round(1.0*pred_tp/(true_p+0.00001),2)

        ret_p.append(p)
        ret_r.append(r)
        ret_t.append(t)
    #end for t

    return ret_p, ret_r, ret_t

y_mod = label_binarize(y,classes=[0,1,2])

print (y_mod.shape)

# Author: Kalidas Y <ykalidas at iittp dot ac dot in>
# License: BSD 3 clause

def myplot(myclass,mycolor) :
    plt.figure()
    p, r, t = my_pr_curve(y_mod[:,myclass],y_pred_proba[:,myclass])
    plt.xticks(np.arange(0,1.1,0.1))
    plt.grid()
    plt.plot(r,p,'*',color=mycolor)

myplot(0,'red')
myplot(1,'blue')
myplot(2,'green')

plt.show()