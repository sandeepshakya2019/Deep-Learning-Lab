{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23238af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super().__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T  # unnormalized attention weights\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / self.d_out_kq**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb582470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [SelfAttention(d_in, d_out_kq, d_out_v)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5771fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super().__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "\n",
    "    def forward(self, x_1, x_2):           # x_2 is new\n",
    "        queries_1 = x_1 @ self.W_query\n",
    "\n",
    "        keys_2 = x_2 @ self.W_key          # new\n",
    "        values_2 = x_2 @ self.W_value      # new\n",
    "\n",
    "        attn_scores = queries_1 @ keys_2.T # new\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / self.d_out_kq**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values_2\n",
    "        return context_vec"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
