{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29d2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 1000  # Suppose vocab size is 1000\n",
    "embed_dim = 64     # Embedding dimension\n",
    "num_heads = 4      # Number of attention heads\n",
    "num_layers = 2     # Number of Transformer blocks\n",
    "block_size = 128   # Max sequence length\n",
    "ffn_hidden = 256   # Hidden layer size in feed-forward network\n",
    "\n",
    "# 1. Self Attention Head\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B,T,head_size)\n",
    "        q = self.query(x) # (B,T,head_size)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v  # (B,T,head_size)\n",
    "        return out\n",
    "\n",
    "# 2. Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([SelfAttention(embed_dim, head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "# 3. Feed Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_hidden):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_hidden, embed_dim),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 4. Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_hidden):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffwd = FeedForward(embed_dim, ffn_hidden)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# 5. GPT Model\n",
    "class SimpleGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, embed_dim)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(embed_dim, num_heads, ffn_hidden) for _ in range(num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)  # final layer norm\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)  # output head\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)      # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T,C)\n",
    "        x = tok_emb + pos_emb                          # (B,T,C)\n",
    "\n",
    "        x = self.blocks(x)                             # (B,T,C)\n",
    "        x = self.ln_f(x)                               # (B,T,C)\n",
    "        logits = self.head(x)                          # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(-1, logits.size(-1))  # (B*T,vocab_size)\n",
    "            targets = targets.view(-1)                 # (B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "# Create model\n",
    "model = SimpleGPT().to(device)\n",
    "print(f\"Simple GPT Model has {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters.\")\n",
    "\n",
    "# Dummy Input Example\n",
    "idx = torch.randint(0, vocab_size, (2, 20)).to(device)  # (batch_size, sequence_length)\n",
    "targets = torch.randint(0, vocab_size, (2, 20)).to(device)\n",
    "logits, loss = model(idx, targets)\n",
    "print(\"Logits shape:\", logits.shape)  # (2, 20, vocab_size)\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
