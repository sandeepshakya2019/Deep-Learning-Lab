{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sefIFe91jrs8"
      },
      "source": [
        "# Lab 8_1 Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU49q4xqjch_",
        "outputId": "bb7766d3-632a-4506-a6cd-e44bbbefe6ac"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "import itertools\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for cat in ['news']:\n",
        "    for text_id in brown.fileids(cat):\n",
        "        raw_text = list(itertools.chain.from_iterable(brown.sents(text_id)))\n",
        "        text = ' '.join(raw_text)\n",
        "        text = text.lower()\n",
        "        text.replace('\\n', ' ')\n",
        "        text = re.sub('[^a-z ]+', '', text)\n",
        "        corpus.append([w for w in text.split() if w != ''])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWQ6KxQxkUbO"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import random, math\n",
        "\n",
        "def subsample_frequent_words(corpus):\n",
        "    filtered_corpus = []\n",
        "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
        "    sum_word_counts = sum(list(word_counts.values()))\n",
        "    word_counts = {word: word_counts[word]/float(sum_word_counts) for word in word_counts}\n",
        "    for text in corpus:\n",
        "        filtered_corpus.append([])\n",
        "        for word in text:\n",
        "            if random.random() < (1+math.sqrt(word_counts[word] * 1e3)) * 1e-3 / float(word_counts[word]):\n",
        "                filtered_corpus[-1].append(word)\n",
        "    return filtered_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s-6wvjrkUzT"
      },
      "outputs": [],
      "source": [
        "corpus = subsample_frequent_words(corpus)\n",
        "vocabulary = set(itertools.chain.from_iterable(corpus))\n",
        "\n",
        "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whH9ow4rkjIp",
        "outputId": "c1a98af4-6a7f-417d-847a-796e46adcdae"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "context_tuple_list = []\n",
        "w = 4\n",
        "\n",
        "for text in corpus:\n",
        "    for i, word in enumerate(text):\n",
        "        first_context_word_index = max(0,i-w)\n",
        "        last_context_word_index = min(i+w, len(text))\n",
        "        for j in range(first_context_word_index, last_context_word_index):\n",
        "            if i!=j:\n",
        "                context_tuple_list.append((word, text[j]))\n",
        "print(\"There are {} pairs of target and context words\".format(len(context_tuple_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMWogjMz5a92"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# Set device to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "context_tensor_list = []\n",
        "target_tensor_list = []\n",
        "\n",
        "# Create context tensors and target tensors, then move them to GPU\n",
        "for target, context in context_tuple_list:\n",
        "    target_tensor = torch.LongTensor([word_to_index[target]])\n",
        "    context_tensor = torch.LongTensor([word_to_index[context]])\n",
        "    target_tensor_list.append(target_tensor)\n",
        "    context_tensor_list.append(context_tensor)\n",
        "\n",
        "# Convert the lists to tensors\n",
        "target_tensor = torch.cat(target_tensor_list, dim=0).to(device)\n",
        "context_tensor = torch.cat(context_tensor_list, dim=0).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlLE5rU7klb9"
      },
      "outputs": [],
      "source": [
        "import torch.nn  as  nn\n",
        "import torch.autograd  as autograd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_size, vocab_size):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
        "\n",
        "    def forward(self, context_word):\n",
        "        emb = self.embeddings(context_word)\n",
        "        hidden = self.linear(emb)\n",
        "        out = F.log_softmax(hidden, dim=1)  # Specify dim=1 to apply over vocab size\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAA4oIhxlTaz"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping():\n",
        "    def __init__(self, patience=5, min_percent_gain=0.1):\n",
        "        self.patience = patience\n",
        "        self.loss_list = []\n",
        "        self.min_percent_gain = min_percent_gain / 100.\n",
        "\n",
        "    def update_loss(self, loss):\n",
        "        self.loss_list.append(loss)\n",
        "        if len(self.loss_list) > self.patience:\n",
        "            del self.loss_list[0]\n",
        "\n",
        "    def stop_training(self):\n",
        "        gain = (max(self.loss_list) - min(self.loss_list)) / max(self.loss_list)\n",
        "        print(\", Loss gain: {}%\".format(round(100*gain,2)))\n",
        "        if len(self.loss_list) == 1:\n",
        "            return False\n",
        "        if gain < self.min_percent_gain:\n",
        "            return True\n",
        "        else:\n",
        "            return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udHpQ7JasqV7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Vocabulary size and initialization\n",
        "vocabulary_size = len(vocabulary)\n",
        "\n",
        "# Max no. of epochs\n",
        "epochs = 100\n",
        "\n",
        "# Batch size (modify this according to your preference)\n",
        "batch_size = 1024\n",
        "\n",
        "# Create the Word2Vec model and move it to the GPU\n",
        "net = Word2Vec(embedding_size=2, vocab_size=vocabulary_size).to(device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "early_stopping = EarlyStopping(patience=4, min_percent_gain=0.2)\n",
        "\n",
        "# Create a DataLoader for batching\n",
        "dataset = TensorDataset(context_tensor, target_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC2PF0Rfn3DR",
        "outputId": "f1d1c597-4686-47ec-ae58-d31011944550"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()  # Start time for epoch\n",
        "\n",
        "    losses = []\n",
        "    for context_batch, target_batch in dataloader:\n",
        "        net.zero_grad()\n",
        "\n",
        "        # Forward pass (context_batch and target_batch are already on GPU)\n",
        "        log_probs = net(context_batch)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_function(log_probs, target_batch)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.data.cpu().numpy())  # Move loss to CPU for printing\n",
        "\n",
        "    # Calculate elapsed time\n",
        "    epoch_time = time.time() - start_time  # Time taken for this epoch\n",
        "\n",
        "    # Print the epoch number, time taken, and loss with 4 decimal precision\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Time: {epoch_time:.4f}s, Loss: {np.mean(losses):.5f}\", end=\"\")\n",
        "\n",
        "    # Update early stopping\n",
        "    early_stopping.update_loss(np.mean(losses))\n",
        "\n",
        "    # Check for early stopping condition\n",
        "    if early_stopping.stop_training():\n",
        "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_YfMYDZljvb"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def get_batches(context_tuple_list, batch_size=128):\n",
        "    random.shuffle(context_tuple_list)\n",
        "    batches = []\n",
        "    batch_target, batch_context, batch_negative = [], [], []\n",
        "    for i in range(len(context_tuple_list)):\n",
        "        batch_target.append(word_to_index[context_tuple_list[i][0]])\n",
        "        batch_context.append(word_to_index[context_tuple_list[i][1]])\n",
        "        batch_negative.append([word_to_index[w] for w in context_tuple_list[i][2]])\n",
        "        if (i+1) % batch_size == 0 or i == len(context_tuple_list)-1:\n",
        "            tensor_target = autograd.Variable(torch.from_numpy(np.array(batch_target)).long())\n",
        "            tensor_context = autograd.Variable(torch.from_numpy(np.array(batch_context)).long())\n",
        "            tensor_negative = autograd.Variable(torch.from_numpy(np.array(batch_negative)).long())\n",
        "            batches.append((tensor_target, tensor_context, tensor_negative))\n",
        "            batch_target, batch_context, batch_negative = [], [], []\n",
        "    return batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU3LqKjUlweS"
      },
      "outputs": [],
      "source": [
        "from numpy.random import multinomial\n",
        "import time\n",
        "\n",
        "def sample_negative(sample_size):\n",
        "    sample_probability = {}\n",
        "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
        "    normalizing_factor = sum([v**0.75 for v in word_counts.values()])\n",
        "    for word in word_counts:\n",
        "        sample_probability[word] = word_counts[word]**0.75 / normalizing_factor\n",
        "    words = np.array(list(word_counts.keys()))\n",
        "    while True:\n",
        "        word_list = []\n",
        "        sampled_index = np.array(multinomial(sample_size, list(sample_probability.values())))\n",
        "        for index, count in enumerate(sampled_index):\n",
        "            word_list.extend([words[index]] * count)  # Extend list efficiently\n",
        "        yield word_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4o4K1eLmT1n",
        "outputId": "a622f637-3703-480b-db28-85e1840b64b5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "context_tuple_list = []\n",
        "w = 4\n",
        "negative_samples = sample_negative(8)\n",
        "\n",
        "for text in corpus:\n",
        "    for i, word in enumerate(text):\n",
        "        first_context_word_index = max(0,i-w)\n",
        "        last_context_word_index = min(i+w, len(text))\n",
        "        for j in range(first_context_word_index, last_context_word_index):\n",
        "            if i!=j:\n",
        "                context_tuple_list.append((word, text[j], next(negative_samples)))\n",
        "print(\"There are {} pairs of target and context words\".format(len(context_tuple_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1QeWT27l25i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_size, vocab_size):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.embeddings_target = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.embeddings_context = nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "    def forward(self, target_word, context_word, negative_example):\n",
        "        emb_target = self.embeddings_target(target_word)\n",
        "        emb_context = self.embeddings_context(context_word)\n",
        "        emb_product = torch.mul(emb_target, emb_context)\n",
        "        emb_product = torch.sum(emb_product, dim=1)\n",
        "        out = torch.sum(F.logsigmoid(emb_product))\n",
        "        emb_negative = self.embeddings_context(negative_example)\n",
        "        emb_product = torch.bmm(emb_negative, emb_target.unsqueeze(2))\n",
        "        emb_product = torch.sum(emb_product, dim=1)\n",
        "        out += torch.sum(F.logsigmoid(-emb_product))\n",
        "        return -out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BITqAWbt0sA",
        "outputId": "ad4f75cb-e183-4460-d32e-b383af256cad"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "net = Word2Vec(embedding_size=200, vocab_size=vocabulary_size)\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "early_stopping = EarlyStopping(patience=5, min_percent_gain=1)\n",
        "\n",
        "i=0\n",
        "while i<5:\n",
        "    i+=1\n",
        "    losses = []\n",
        "    context_tuple_batches = get_batches(context_tuple_list, batch_size=2000)\n",
        "    for i in range(len(context_tuple_batches)):\n",
        "        net.zero_grad()\n",
        "        target_tensor, context_tensor, negative_tensor = context_tuple_batches[i]\n",
        "        loss = net(target_tensor, context_tensor, negative_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.data)\n",
        "    print(\"Loss: \", np.mean(losses))\n",
        "    early_stopping.update_loss(np.mean(losses))\n",
        "    if early_stopping.stop_training():\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfvdl7SGt5VU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_closest_word(word, topn=5):\n",
        "    word_distance = []\n",
        "    emb = net.embeddings_target\n",
        "    pdist = nn.PairwiseDistance()\n",
        "    i = word_to_index[word]\n",
        "    lookup_tensor_i = torch.tensor([i], dtype=torch.long)\n",
        "    v_i = emb(lookup_tensor_i)\n",
        "    for j in range(len(vocabulary)):\n",
        "        if j != i:\n",
        "            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
        "            v_j = emb(lookup_tensor_j)\n",
        "            word_distance.append((index_to_word[j], float(pdist(v_i, v_j))))\n",
        "    word_distance.sort(key=lambda x: x[1])\n",
        "    return word_distance[:topn]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1DnUCUp2bAK",
        "outputId": "33db7adc-cb34-463a-a591-b0fa25f7f40a"
      },
      "outputs": [],
      "source": [
        "print(\"Closest words to 'the':\", get_closest_word('the'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
